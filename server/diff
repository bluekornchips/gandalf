diff --git a/.github/workflows/python-checks.yaml b/.github/workflows/python-checks.yaml
index cc7caa1..bee5a80 100644
--- a/.github/workflows/python-checks.yaml
+++ b/.github/workflows/python-checks.yaml
@@ -74,8 +74,9 @@ jobs:
 
       - name: Run tests and save coverage summary
         run: |
+          set -e
           cd server
-          pytest --cov=src --cov-report=term --cov-report=xml > result.log
+          pytest --cov=src --cov-report=term --cov-report=xml | tee result.log
           tail -n 30 result.log > coverage-summary.txt
 
       - name: Read coverage summary
diff --git a/server/src/config/weights.py b/server/src/config/weights.py
index b91d4d6..eee74aa 100644
--- a/server/src/config/weights.py
+++ b/server/src/config/weights.py
@@ -22,13 +22,7 @@ class WeightsConfig:
     """Schema-validated weights configuration using custom validation."""
 
     def __init__(self, config_dir: Path | None = None, validate: bool = True):
-        """Initialize the weights configuration.
-
-        Args:
-            config_dir: Directory containing gandalf-weights.yaml.
-                        Defaults to using the global weights file system.
-            validate: Whether to validate configuration on load (default: True)
-        """
+        """Initialize the weights configuration."""
         self.config_dir = config_dir
         self.validation_errors: list[str] = []
         self.is_valid = True
@@ -67,15 +61,7 @@ class WeightsConfig:
         return {}
 
     def get(self, path: str, default: Any = None) -> Any:
-        """Get a value from the config using dot notation.
-
-        Args:
-            path: Dot-separated path
-            default: Default value if not found (defaults to CONTEXT_MIN_SCORE)
-
-        Returns:
-            The value, default, or CONTEXT_MIN_SCORE if default is None
-        """
+        """Get a value from the config using dot notation."""
         if default is None:
             default = CONTEXT_MIN_SCORE
 
@@ -88,14 +74,7 @@ class WeightsConfig:
         return value
 
     def get_dict(self, path: str) -> dict[str, Any]:
-        """Get a dictionary from the config using dot notation.
-
-        Args:
-            path: Dot-separated path to dictionary
-
-        Returns:
-            Dictionary or empty dict if not found
-        """
+        """Get a dictionary from the config using dot notation."""
         value = self.get(path)
         return value if isinstance(value, dict) else {}
 
@@ -108,31 +87,19 @@ class WeightsConfig:
         return self.validation_errors
 
     def get_file_extension_weights(self) -> dict[str, float]:
-        """Get file extension priority weights with dot prefixes.
-
-        Returns:
-            Dictionary mapping file extensions to weight values
-        """
+        """Get file extension priority weights with dot prefixes."""
         weights_dict = self.get_dict("file_extensions")
         # Add dot prefixes and convert to float
         return {f".{ext}": float(weight) for ext, weight in weights_dict.items()}
 
     def get_directory_priority_weights(self) -> dict[str, float]:
-        """Get directory importance scores.
-
-        Returns:
-            Dictionary mapping directory names to weight values
-        """
+        """Get directory importance scores."""
         weights_dict = self.get_dict("directories")
         # Convert to float
         return {dir_name: float(weight) for dir_name, weight in weights_dict.items()}
 
     def get_weights_validation_status(self) -> dict[str, Any]:
-        """Get validation status of the weights configuration.
-
-        Returns:
-            Dictionary with validation status information
-        """
+        """Get validation status of the weights configuration."""
         has_errors = self.has_validation_errors()
         error_count = len(self.validation_errors)
 
diff --git a/server/src/core/conversation_filtering.py b/server/src/core/conversation_filtering.py
index bbaeb3b..084ded1 100644
--- a/server/src/core/conversation_filtering.py
+++ b/server/src/core/conversation_filtering.py
@@ -27,12 +27,7 @@ class ConversationFilter:
     """Simple conversation filtering based on keyword matching."""
 
     def __init__(self, project_root: Path, user_prompt: str | None = None):
-        """Initialize the conversation filter.
-
-        Args:
-            project_root: Project root directory for context generation
-            user_prompt: Optional user prompt for keyword extraction
-        """
+        """Initialize the conversation filter."""
         self.project_root = project_root
         self.user_prompt = user_prompt
         self.base_keywords = generate_shared_context_keywords(project_root)
@@ -186,17 +181,7 @@ def apply_conversation_filtering(
     requested_limit: int = 20,
     user_prompt: str | None = None,
 ) -> tuple[list[dict[str, Any]], dict[str, Any]]:
-    """Apply conversation filtering and return filtered conversations with metadata.
-
-    Args:
-        conversations: List of conversations to filter
-        project_root: Project root directory
-        requested_limit: Maximum number of conversations to return
-        user_prompt: Optional user prompt for keyword extraction
-
-    Returns:
-        Tuple of (filtered_conversations, filtering_metadata)
-    """
+    """Apply conversation filtering and return filtered conversations with metadata."""
 
     if not CONVERSATION_FILTERING_ENABLED:
         limited_conversations = conversations[:requested_limit]
diff --git a/server/src/core/server.py b/server/src/core/server.py
index 264d5c9..527796e 100644
--- a/server/src/core/server.py
+++ b/server/src/core/server.py
@@ -33,6 +33,7 @@ from src.tool_calls.project_operations import (
 )
 from src.utils.access_control import AccessValidator
 from src.utils.common import log_error, log_info
+from src.utils.database_pool import DatabaseService
 from src.utils.jsonrpc import (
     create_error_response,
     create_success_response,
@@ -79,6 +80,9 @@ class GandalfMCP:
         self.tool_handlers = TOOL_HANDLERS
         self.is_ready = False
 
+        self.db_service = DatabaseService()
+        self.db_service.initialize()
+
         # Request handlers
         self.handlers = {
             "initialize": self._initialize,
@@ -292,3 +296,9 @@ class GandalfMCP:
 
         message_loop = MessageLoopHandler(self)
         message_loop.run_message_loop()
+
+    def shutdown(self):
+        """Shutdown the server and cleanup resources."""
+        if self.db_service:
+            self.db_service.shutdown()
+            log_info("Server shutdown completed")
diff --git a/server/src/main.py b/server/src/main.py
index cead1f1..76bcf2f 100644
--- a/server/src/main.py
+++ b/server/src/main.py
@@ -45,7 +45,10 @@ def main() -> None:
 
     try:
         server = GandalfMCP(project_root=project_root)
-        server.run()
+        try:
+            server.run()
+        finally:
+            server.shutdown()
     except Exception as e:
         print(f"Error: Failed to start server: {e}", file=sys.stderr)
         sys.exit(1)
diff --git a/server/src/tool_calls/aggregator.py b/server/src/tool_calls/aggregator.py
index 6f7eade..e886009 100644
--- a/server/src/tool_calls/aggregator.py
+++ b/server/src/tool_calls/aggregator.py
@@ -447,40 +447,7 @@ def handle_recall_conversations(
     search_query: str | None = None,
     tags: list[str] | None = None,
 ) -> dict[str, Any]:
-    """
-    Cross-platform conversation recall that aggregates results from all available tools.
-
-    This function detects available tools and combines their conversation data
-    into a unified format for comprehensive context analysis with intelligent filtering.
-
-    When search_query or tags are provided, it filters conversations for
-    specific content instead of returning all contextually relevant recent
-    conversations.
-
-    Args:
-        fast_mode: Use fast extraction vs comprehensive analysis
-        days_lookback: Number of days to look back for conversations
-        limit: Maximum number of conversations to return (used as context limit
-               for intelligent filtering)
-        min_score: Minimum relevance score threshold
-        conversation_types: Filter by conversation types
-        tools: Filter by specific agentic tools (e.g. ["windsurf", "cursor"])
-        project_root: Project root directory for context generation
-        user_prompt: Optional user prompt for dynamic keyword extraction
-        search_query: Optional query to filter conversations for specific content
-        tags: Optional list of tags/keywords to filter conversations
-
-    Returns:
-        Dict containing aggregated conversation data from all detected tools
-        with intelligent filtering
-
-    Note:
-        This function automatically detects and processes conversations from
-        multiple tools for comprehensive conversation context.
-        When intelligent filtering is enabled, the limit parameter serves as a
-        context limit rather than a hard limit, allowing for quality-based
-        filtering.
-    """
+    """Cross-platform conversation recall that aggregates results from all available tools."""
     import time
 
     start_time = time.time()
diff --git a/server/src/tool_calls/export.py b/server/src/tool_calls/export.py
index a92bd6a..6df5d9e 100644
--- a/server/src/tool_calls/export.py
+++ b/server/src/tool_calls/export.py
@@ -21,14 +21,7 @@ from src.utils.cursor_chat_query import CursorQuery, list_cursor_workspaces
 
 
 def format_timestamp(timestamp: float | None = None) -> str:
-    """Format timestamp for filenames and content.
-
-    Args:
-        timestamp: Unix timestamp (defaults to current time)
-
-    Returns:
-        Formatted timestamp string
-    """
+    """Format timestamp for filenames and content."""
     if timestamp is None:
         timestamp = datetime.now().timestamp()
 
@@ -39,14 +32,7 @@ def format_timestamp(timestamp: float | None = None) -> str:
 
 
 def sanitize_filename(filename: str) -> str:
-    """Sanitize filename for filesystem safety.
-
-    Args:
-        filename: Raw filename
-
-    Returns:
-        Sanitized filename safe for filesystem use
-    """
+    """Sanitize filename for filesystem safety."""
     # Remove or replace problematic characters
     sanitized = re.sub(FILENAME_INVALID_CHARS_PATTERN, "_", filename)
 
@@ -66,16 +52,7 @@ def sanitize_filename(filename: str) -> str:
 def handle_export_individual_conversations(
     arguments: dict[str, Any], project_root: Path, **kwargs
 ) -> dict[str, Any]:
-    """Export individual conversations to files.
-
-    Args:
-        arguments: Tool arguments containing format, output_dir, etc.
-        project_root: Project root path
-        **kwargs: Additional arguments
-
-    Returns:
-        MCP response with export results
-    """
+    """Export individual conversations to files."""
     try:
         # Validate arguments
         format_type = arguments.get("format", "json")
@@ -203,16 +180,7 @@ def handle_export_individual_conversations(
 def handle_list_cursor_workspaces(
     arguments: dict[str, Any], project_root: Path, **kwargs
 ) -> dict[str, Any]:
-    """List available Cursor workspaces.
-
-    Args:
-        arguments: Tool arguments (currently unused)
-        project_root: Project root path
-        **kwargs: Additional arguments
-
-    Returns:
-        MCP response with workspace list
-    """
+    """List available Cursor workspaces."""
     try:
         result = list_cursor_workspaces()
         return AccessValidator.create_success_response(
diff --git a/server/src/utils/access_control.py b/server/src/utils/access_control.py
index 4403990..78dc666 100644
--- a/server/src/utils/access_control.py
+++ b/server/src/utils/access_control.py
@@ -44,18 +44,7 @@ class AccessValidator:
         max_length: int | None = None,
         required: bool = True,
     ) -> tuple[bool, str]:
-        """Validate string input with length and content constraints.
-
-        Args:
-            value: Input value to validate
-            field_name: Name of the field for error messages
-            min_length: Minimum required length
-            max_length: Maximum allowed length (defaults to MAX_STRING_LENGTH)
-            required: Whether the field is required
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate string input with length and content constraints."""
         if required and not value:
             return False, f"{field_name} is required"
 
@@ -89,18 +78,7 @@ class AccessValidator:
         item_type: type | None = None,
         required: bool = True,
     ) -> tuple[bool, str]:
-        """Validate array input with type and size constraints.
-
-        Args:
-            value: Input value to validate
-            field_name: Name of the field for error messages
-            max_items: Maximum allowed items (defaults to MAX_ARRAY_LENGTH)
-            item_type: Required type for array items
-            required: Whether the field is required
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate array input with type and size constraints."""
         if required and not value:
             return False, f"{field_name} is required"
 
@@ -133,18 +111,7 @@ class AccessValidator:
         max_value: int | None = None,
         required: bool = True,
     ) -> tuple[bool, str]:
-        """Validate integer input with range constraints.
-
-        Args:
-            value: Input value to validate
-            field_name: Name of the field for error messages
-            min_value: Minimum allowed value
-            max_value: Maximum allowed value
-            required: Whether the field is required
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate integer input with range constraints."""
         if required and value is None:
             return False, f"{field_name} is required"
 
@@ -170,17 +137,7 @@ class AccessValidator:
         valid_values: list[str],
         required: bool = True,
     ) -> tuple[bool, str]:
-        """Validate enum input against allowed values.
-
-        Args:
-            value: Input value to validate
-            field_name: Name of the field for error messages
-            valid_values: List of allowed values
-            required: Whether the field is required
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate enum input against allowed values."""
         if required and not value:
             return False, f"{field_name} is required"
 
@@ -202,15 +159,7 @@ class AccessValidator:
     def validate_path(
         cls, path: str | Path, field_name: str = "path"
     ) -> tuple[bool, str]:
-        """Validate file path for security.
-
-        Args:
-            path: Path to validate
-            field_name: Name of the field for error messages
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate file path for security."""
         path_str = str(path)
 
         if cls._check_for_tricks(path_str):
@@ -240,14 +189,7 @@ class AccessValidator:
 
     @classmethod
     def validate_file_extension(cls, extension: str) -> tuple[bool, str]:
-        """Validate file extension for security using blocklist approach.
-
-        Args:
-            extension: File extension to validate
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate file extension for security using blocklist approach."""
         if not extension.startswith("."):
             extension = f".{extension}"
 
@@ -295,14 +237,7 @@ class AccessValidator:
 
     @classmethod
     def create_error_response(cls, message: str) -> dict[str, Any]:
-        """Create a standardized MCP error response.
-
-        Args:
-            message: Error message
-
-        Returns:
-            MCP error response dictionary
-        """
+        """Create a standardized MCP error response."""
         return {
             "isError": True,
             "error": message,
@@ -311,14 +246,7 @@ class AccessValidator:
 
     @classmethod
     def create_success_response(cls, text: str) -> dict[str, Any]:
-        """Create a standardized MCP success response.
-
-        Args:
-            text: Response text
-
-        Returns:
-            MCP success response dictionary
-        """
+        """Create a standardized MCP success response."""
         return {"content": [{"type": "text", "text": text}]}
 
     @classmethod
@@ -330,18 +258,7 @@ class AccessValidator:
         max_length: int | None = None,
         required: bool = True,
     ) -> tuple[bool, str]:
-        """Validate conversation content with enhanced security checks.
-
-        Args:
-            value: Content to validate
-            field_name: Name of the field for error messages
-            min_length: Minimum required length
-            max_length: Maximum allowed length
-            required: Whether the field is required
-
-        Returns:
-            Tuple of (is_valid, error_message)
-        """
+        """Validate conversation content with enhanced security checks."""
         is_valid, error_msg = cls.validate_string(
             value, field_name, min_length, max_length, required
         )
@@ -356,17 +273,7 @@ class AccessValidator:
 
     @classmethod
     def _check_for_conversation_tricks(cls, text: str) -> bool:
-        """Check for conversation-specific dangerous patterns.
-
-        Validates conversation content for potentially malicious patterns,
-        while being less restrictive than general input validation.
-
-        Args:
-            text: Conversation text to validate
-
-        Returns:
-            bool: True if dangerous patterns are found, False otherwise
-        """
+        """Check for conversation-specific dangerous patterns."""
         for pattern in CONVERSATION_DANGEROUS_PATTERNS:
             if re.search(pattern, text, re.IGNORECASE | re.DOTALL):
                 return True
@@ -375,14 +282,7 @@ class AccessValidator:
 
     @classmethod
     def sanitize_project_name(cls, project_name: str) -> str:
-        """Sanitize project name for filesystem safety with transparency.
-
-        Args:
-            project_name: Raw project name to sanitize
-
-        Returns:
-            str: Sanitized project name safe for filesystem use
-        """
+        """Sanitize project name for filesystem safety with transparency."""
         if not project_name:
             log_debug("Empty project name provided, using default")
             return "unnamed_project"
@@ -420,28 +320,14 @@ class AccessValidator:
 
 
 def validate_conversation_id(conv_id: Any) -> tuple[bool, str]:
-    """Validate conversation ID with specific rules.
-
-    Args:
-        conv_id: Conversation ID to validate
-
-    Returns:
-        Tuple of (is_valid, error_message)
-    """
+    """Validate conversation ID with specific rules."""
     return AccessValidator.validate_string(
         conv_id, "conversation_id", min_length=1, max_length=100
     )
 
 
 def validate_search_query(query: Any) -> tuple[bool, str]:
-    """Validate search query with specific rules.
-
-    Args:
-        query: Search query to validate
-
-    Returns:
-        Tuple of (is_valid, error_message)
-    """
+    """Validate search query with specific rules."""
     return AccessValidator.validate_string(
         query,
         "query",
@@ -451,14 +337,7 @@ def validate_search_query(query: Any) -> tuple[bool, str]:
 
 
 def validate_file_types(file_types: Any) -> tuple[bool, str]:
-    """Validate file types array with extension validation.
-
-    Args:
-        file_types: Array of file extensions to validate
-
-    Returns:
-        Tuple of (is_valid, error_message)
-    """
+    """Validate file types array with extension validation."""
     is_valid, error = AccessValidator.validate_array(
         file_types,
         "file_types",
@@ -479,14 +358,7 @@ def validate_file_types(file_types: Any) -> tuple[bool, str]:
 
 
 def get_platform_blocked_paths(platform: str | None = None) -> set:
-    """Get platform-specific blocked paths.
-
-    Args:
-        platform: Platform identifier (linux, macos, wsl)
-
-    Returns:
-        Set of blocked paths for the platform
-    """
+    """Get platform-specific blocked paths."""
     if platform == "linux":
         return COMMON_BLOCKED_PATHS | LINUX_SPECIFIC_BLOCKED_PATHS
     elif platform == "macos":
diff --git a/server/src/utils/conversation_export.py b/server/src/utils/conversation_export.py
index e412961..3288372 100644
--- a/server/src/utils/conversation_export.py
+++ b/server/src/utils/conversation_export.py
@@ -19,19 +19,7 @@ def export_conversations_simple(
     format_type: str = CONVERSATION_EXPORT_FORMAT_DEFAULT,
     silent: bool = False,
 ) -> bool:
-    """
-    Simple export of all conversations to a single file.
-
-    For individual conversation exports, use the MCP tool 'export_individual_conversations'.
-
-    Args:
-        output_path: Path where to save the exported data
-        format_type: Export format - 'json', 'markdown', or 'cursor'
-        silent: Suppress console output
-
-    Returns:
-        True if export succeeded, False otherwise
-    """
+    """Simple export of all conversations to a single file."""
     if format_type not in CONVERSATION_EXPORT_FORMATS:
         raise ValueError(
             f"format_type must be one of: {', '.join(CONVERSATION_EXPORT_FORMATS)}"
@@ -57,15 +45,7 @@ def export_conversations_simple(
 
 
 def list_workspaces(silent: bool = False) -> list[str]:
-    """
-    List available workspace hashes.
-
-    Args:
-        silent: Suppress console output
-
-    Returns:
-        List of workspace hash strings
-    """
+    """List available workspace hashes."""
     try:
         query_tool = CursorQuery(silent=silent)
         data = query_tool.query_all_conversations()
diff --git a/server/src/utils/database_pool.py b/server/src/utils/database_pool.py
index cc84aed..c34d8cb 100644
--- a/server/src/utils/database_pool.py
+++ b/server/src/utils/database_pool.py
@@ -106,46 +106,88 @@ class ConnectionPool:
             }
 
 
-# Global connection pool instance
-_connection_pool: ConnectionPool | None = None
-_pool_lock = threading.Lock()
+class DatabaseService:
+    """Database service with explicit initialization and lifecycle management."""
 
+    def __init__(self):
+        """Initialize the database service."""
+        self._pool: ConnectionPool | None = None
+        self._initialized = False
+        self._lock = threading.Lock()
+
+    def initialize(self, max_connections: int = 5, timeout: float = 2.0) -> None:
+        """Initialize the database service with connection pool."""
+        with self._lock:
+            if self._initialized:
+                return
+            self._pool = ConnectionPool(max_connections, timeout)
+            self._initialized = True
+            log_debug("Database service initialized")
+
+    @contextmanager
+    def get_connection(
+        self, db_path: Path
+    ) -> Generator[sqlite3.Connection, None, None]:
+        """Get a database connection from the service pool."""
+        if not self._initialized or not self._pool:
+            raise RuntimeError(
+                "DatabaseService not initialized. Call initialize() first."
+            )
+        with self._pool.get_connection(db_path) as conn:
+            yield conn
+
+    def get_pool_stats(self) -> dict[str, int]:
+        """Get current pool statistics."""
+        if not self._pool:
+            return {}
+        return self._pool.get_pool_stats()
+
+    def is_initialized(self) -> bool:
+        """Check if the database service is initialized."""
+        return self._initialized
+
+    def shutdown(self) -> None:
+        """Shutdown the database service and close all connections."""
+        with self._lock:
+            if self._pool:
+                self._pool.close_all()
+                self._pool = None
+                self._initialized = False
+                log_debug("Database service shutdown completed")
 
-def get_database_pool() -> ConnectionPool:
-    """Get the global database connection pool instance."""
-    global _connection_pool
 
-    if _connection_pool is None:
-        with _pool_lock:
-            if _connection_pool is None:
-                _connection_pool = ConnectionPool()
-                log_debug("Initialized global database connection pool")
+# Backward compatibility functions for existing code
+_default_service: DatabaseService | None = None
+_service_lock = threading.Lock()
 
-    return _connection_pool
+
+def get_database_pool() -> DatabaseService:
+    """Get the default database service instance for backward compatibility."""
+    global _default_service
+
+    if _default_service is None:
+        with _service_lock:
+            if _default_service is None:
+                _default_service = DatabaseService()
+                _default_service.initialize()
+
+    return _default_service
 
 
 def close_database_pool() -> None:
-    """Close the global database connection pool."""
-    global _connection_pool
+    """Close the default database service for backward compatibility."""
+    global _default_service
 
-    if _connection_pool is not None:
-        with _pool_lock:
-            if _connection_pool is not None:
-                _connection_pool.close_all()
-                _connection_pool = None
-                log_debug("Closed global database connection pool")
+    if _default_service is not None:
+        with _service_lock:
+            if _default_service is not None:
+                _default_service.shutdown()
+                _default_service = None
 
 
 @contextmanager
 def get_database_connection(db_path: Path) -> Generator[sqlite3.Connection, None, None]:
-    """Get a database connection using the global pool.
-
-    Args:
-        db_path: Path to the SQLite database
-
-    Yields:
-        SQLite connection object
-    """
-    pool = get_database_pool()
-    with pool.get_connection(db_path) as conn:
+    """Get a database connection using the default service for backward compatibility."""
+    service = get_database_pool()
+    with service.get_connection(db_path) as conn:
         yield conn
diff --git a/server/src/utils/memory_cache.py b/server/src/utils/memory_cache.py
index 1136e12..77b9023 100644
--- a/server/src/utils/memory_cache.py
+++ b/server/src/utils/memory_cache.py
@@ -41,24 +41,7 @@ class MemoryAwareLRUCache:
         self._current_memory = 0
 
     def _estimate_size(self, value: Any) -> int:
-        """Estimate memory size of a value in bytes.
-
-        Provides memory size estimation for different data types to enable
-        accurate memory tracking. Uses optimized size calculation methods
-        for common data types with fallback for complex objects.
-
-        Args:
-            value: The value to estimate size for
-
-        Returns:
-            Estimated size in bytes
-
-        Size Estimation Methods:
-            - String/bytes: Use len() for direct character/byte count
-            - Dict: JSON serialization size as approximation
-            - Other objects: sys.getsizeof() for Python object size
-            - Fallback: 1KB default for complex objects
-        """
+        """Estimate memory size of a value in bytes."""
         try:
             if isinstance(value, str | bytes):
                 return len(value)
@@ -72,20 +55,7 @@ class MemoryAwareLRUCache:
             return 1024
 
     def _check_memory_pressure(self) -> bool:
-        """Check if we're under memory pressure and need to evict entries.
-
-        Performs periodic memory pressure checks to determine if cache cleanup
-        is needed. Uses garbage collection to get accurate memory readings
-        and compares against configured limits.
-
-        Returns:
-            True if memory pressure detected and eviction needed
-
-        Memory Pressure Detection:
-            - Only checks at specified intervals to avoid overhead
-            - Forces garbage collection for accurate memory readings
-            - Compares current cache memory against max_memory_bytes limit
-        """
+        """Check if we're under memory pressure and need to evict entries."""
         now = time.time()
         if now - self._last_memory_check < self.check_interval:
             return False
@@ -99,18 +69,7 @@ class MemoryAwareLRUCache:
         return self._current_memory > self.max_memory_bytes
 
     def _evict_expired(self) -> None:
-        """Remove expired entries based on TTL.
-
-        Scans all cache entries and removes those that have exceeded their
-        time-to-live. This is the first eviction strategy applied when
-        memory pressure is detected.
-
-        Expiration Logic:
-            - Compares current time against entry access time
-            - Removes entries older than ttl_seconds
-            - Updates memory tracking and statistics
-            - Logs eviction count for monitoring
-        """
+        """Remove expired entries based on TTL."""
         now = time.time()
         expired_keys = [
             key
@@ -125,21 +84,7 @@ class MemoryAwareLRUCache:
             log_debug(f"Evicted {len(expired_keys)} expired cache entries")
 
     def _evict_lru(self, target_count: int | None = None) -> None:
-        """Remove least recently used entries when memory pressure persists.
-
-        Implements LRU (Least Recently Used) eviction policy to remove
-        cache entries that haven't been accessed recently. This is applied
-        after TTL-based eviction if memory pressure still exists.
-
-        Args:
-            target_count: Number of entries to evict (default: 25% of cache)
-
-        LRU Eviction Strategy:
-            - Sorts entries by access time (oldest first)
-            - Removes specified number of least recently used entries
-            - Defaults to removing 25% of cache if no target specified
-            - Updates memory tracking and logs eviction statistics
-        """
+        """Remove least recently used entries when memory pressure persists."""
         if target_count is None:
             target_count = max(1, len(self._cache) // 4)  # Remove 25%
 
@@ -159,21 +104,7 @@ class MemoryAwareLRUCache:
             log_debug(f"Evicted {evicted} LRU cache entries")
 
     def _remove_key(self, key: str) -> None:
-        """Remove a key from all internal structures and update memory tracking.
-
-        Safely removes a cache entry from all internal data structures
-        and updates memory usage tracking. This is a low-level method
-        used by eviction and expiration processes.
-
-        Args:
-            key: Cache key to remove
-
-        Cleanup Operations:
-            - Removes entry from main cache dictionary
-            - Cleans up access time tracking
-            - Updates memory usage statistics
-            - Removes size tracking information
-        """
+        """Remove a key from all internal structures and update memory tracking."""
         if key in self._cache:
             self._current_memory -= self._cache_sizes.get(key, 0)
             del self._cache[key]
@@ -231,17 +162,7 @@ class MemoryAwareLRUCache:
             )
 
     def clear(self) -> None:
-        """Clear all cache entries and reset memory tracking.
-
-        Removes all cached entries and resets internal tracking structures.
-        This provides a clean slate for the cache and frees all memory.
-
-        Operations Performed:
-            - Clear main cache dictionary
-            - Reset access time tracking
-            - Clear memory size tracking
-            - Reset total memory counter to zero
-        """
+        """Clear all cache entries and reset memory tracking."""
         with self._lock:
             self._cache.clear()
             self._access_times.clear()
@@ -250,25 +171,7 @@ class MemoryAwareLRUCache:
             log_debug("Cleared all cache entries")
 
     def get_stats(self) -> dict[str, Any]:
-        """Get comprehensive cache statistics for monitoring and debugging.
-
-        Returns detailed statistics about cache usage, memory consumption,
-        and utilization rates. Useful for performance monitoring and tuning.
-
-        Returns:
-            Dictionary containing cache statistics:
-            - total_items: Current number of cached entries
-            - memory_usage_mb: Current memory usage in megabytes
-            - memory_limit_mb: Configured memory limit in megabytes
-            - items_limit: Maximum number of items allowed
-            - memory_utilization: Memory usage as percentage of limit (0.0-1.0)
-            - items_utilization: Item count as percentage of limit (0.0-1.0)
-
-        Example:
-            stats = cache.get_stats()
-            print(f"Memory usage: {stats['memory_usage_mb']:.1f}MB")
-            print(f"Cache utilization: {stats['memory_utilization']:.1%}")
-        """
+        """Get comprehensive cache statistics for monitoring and debugging."""
         with self._lock:
             return {
                 "total_items": len(self._cache),
@@ -280,22 +183,7 @@ class MemoryAwareLRUCache:
             }
 
     def cleanup_if_needed(self) -> None:
-        """Perform cache cleanup if memory pressure is detected.
-
-        Provides a convenient method to trigger cache cleanup when needed.
-        This can be called periodically or when the application detects
-        memory pressure to proactively manage cache size.
-
-        Cleanup Strategy:
-            1. Check for memory pressure using configured thresholds
-            2. Remove expired entries first (TTL-based cleanup)
-            3. Apply LRU eviction if memory usage still high (>80% of limit)
-            4. Log cleanup actions for monitoring
-
-        Usage:
-            Call this method periodically in long-running applications
-            or when memory usage warnings are detected.
-        """
+        """Perform cache cleanup if memory pressure is detected."""
         with self._lock:
             if self._check_memory_pressure():
                 log_info("Memory pressure detected, performing cache cleanup")
@@ -311,23 +199,7 @@ _cache_lock = threading.Lock()
 
 
 def get_conversation_cache() -> MemoryAwareLRUCache:
-    """Get the global conversation cache instance with optimized settings.
-
-    Returns a singleton cache instance configured specifically for conversation
-    data with appropriate memory limits and TTL settings for conversation content.
-
-    Returns:
-        Global conversation cache instance
-
-    Cache Configuration:
-        - Memory Limit: 80MB (optimized for conversation data)
-        - Item Limit: 500 conversations
-        - TTL: 30 minutes (balances freshness and performance)
-
-    Thread Safety:
-        Uses double-checked locking pattern to ensure thread-safe
-        singleton initialization.
-    """
+    """Get the global conversation cache instance with optimized settings."""
     global _conversation_cache
 
     if _conversation_cache is None:
@@ -344,24 +216,7 @@ def get_conversation_cache() -> MemoryAwareLRUCache:
 
 
 def get_keyword_cache() -> MemoryAwareLRUCache:
-    """Get the global keyword cache instance with keyword-optimized settings.
-
-    Returns a singleton cache instance configured specifically for keyword
-    and project metadata with settings optimized for smaller, frequently
-    accessed data.
-
-    Returns:
-        Global keyword cache instance
-
-    Cache Configuration:
-        - Memory Limit: 20MB (smaller for keyword data)
-        - Item Limit: 200 keyword sets
-        - TTL: 1 hour (longer for relatively stable keyword data)
-
-    Thread Safety:
-        Uses double-checked locking pattern to ensure thread-safe
-        singleton initialization.
-    """
+    """Get the global keyword cache instance with keyword-optimized settings."""
     global _keyword_cache
 
     if _keyword_cache is None:
@@ -378,20 +233,7 @@ def get_keyword_cache() -> MemoryAwareLRUCache:
 
 
 def clear_all_caches() -> None:
-    """Clear all global caches and reset memory usage.
-
-    Provides a convenient way to clear all global cache instances at once.
-    Useful for testing, debugging, or when a complete cache reset is needed.
-
-    Operations:
-        - Clears conversation cache if initialized
-        - Clears keyword cache if initialized
-        - Logs cache clearing action for monitoring
-
-    Note:
-        This does not destroy the cache instances, just clears their contents.
-        The caches will continue to function normally after clearing.
-    """
+    """Clear all global caches and reset memory usage."""
     if _conversation_cache:
         _conversation_cache.clear()
     if _keyword_cache:
diff --git a/server/src/utils/schema_validation.py b/server/src/utils/schema_validation.py
index 9c2df13..71b509d 100644
--- a/server/src/utils/schema_validation.py
+++ b/server/src/utils/schema_validation.py
@@ -46,16 +46,7 @@ def _create_number_field(
     max_val: float = VALIDATION_CONVERSATION_PARAM_MAX,
     default_val: float = DEFAULT_WEIGHT_VALUE,
 ) -> dict[str, Any]:
-    """Create a number field validation schema.
-
-    Args:
-        min_val: Minimum allowed value
-        max_val: Maximum allowed value
-        default_val: Default value if not provided
-
-    Returns:
-        Dictionary containing validation schema for number field
-    """
+    """Create a number field validation schema."""
     return {
         "type": "number",
         "min": min_val,
@@ -68,15 +59,7 @@ def _create_integer_field(
     min_val: int = VALIDATION_FILE_SIZE_MIN,
     default_val: int = DEFAULT_OPTIMAL_FILE_SIZE_MIN,
 ) -> dict[str, Any]:
-    """Create an integer field validation schema.
-
-    Args:
-        min_val: Minimum allowed value
-        default_val: Default value if not provided
-
-    Returns:
-        Dictionary containing validation schema for integer field
-    """
+    """Create an integer field validation schema."""
     return {
         "type": "integer",
         "min": min_val,
@@ -89,16 +72,7 @@ def _create_dict_field(
     required: bool = False,
     default_val: dict[str, Any] | None = None,
 ) -> dict[str, Any]:
-    """Create a dictionary field validation schema.
-
-    Args:
-        schema: Nested schema for dictionary contents
-        required: Whether field is required
-        default_val: Default value if not provided
-
-    Returns:
-        Dictionary containing validation schema for dict field
-    """
+    """Create a dictionary field validation schema."""
     field_def: dict[str, Any] = {
         "type": "dict",
         "schema": schema,
@@ -241,14 +215,7 @@ class GandalfSchemaValidator:
         self.normalized_data: dict[str, Any] = {}
 
     def validate(self, data: dict[str, Any]) -> bool:
-        """Validate data against schema.
-
-        Args:
-            data: Configuration data to validate
-
-        Returns:
-            True if validation passes, False otherwise
-        """
+        """Validate data against schema."""
         self.errors = []
         self.normalized_data = {}
 
@@ -262,16 +229,7 @@ class GandalfSchemaValidator:
     def _validate_recursive(
         self, data: dict[str, Any], schema: dict[str, Any], path: str
     ) -> dict[str, Any]:
-        """Recursively validate and normalize data.
-
-        Args:
-            data: Configuration data to validate
-            schema: Schema definition to validate against
-            path: Current validation path for error reporting
-
-        Returns:
-            Normalized data with defaults applied
-        """
+        """Recursively validate and normalize data."""
         result = {}
 
         # Check for required fields
@@ -311,16 +269,7 @@ class GandalfSchemaValidator:
     def _validate_field(
         self, value: Any, field_schema: dict[str, Any], path: str
     ) -> Any:
-        """Validate a single field.
-
-        Args:
-            value: Field value to validate
-            field_schema: Schema definition for this field
-            path: Current validation path for error reporting
-
-        Returns:
-            Validated and potentially normalized field value
-        """
+        """Validate a single field."""
         # Type validation
         expected_type = field_schema.get("type")
         if expected_type == "number":
@@ -385,24 +334,12 @@ class GandalfSchemaValidator:
         return value
 
     def normalized(self, data: dict[str, Any]) -> dict[str, Any]:
-        """Return normalized data with defaults applied.
-
-        Args:
-            data: Original data (fallback if normalization failed)
-
-        Returns:
-            Normalized data with defaults applied, or original data if normalization
-            failed
-        """
+        """Return normalized data with defaults applied."""
         return self.normalized_data if self.normalized_data else data
 
 
 def create_default_weights_file() -> Path:
-    """Create a default weights configuration file in ~/.gandalf/config.
-
-    Returns:
-        Path to the created default weights file
-    """
+    """Create a default weights configuration file in ~/.gandalf/config."""
     DEFAULT_WEIGHTS_FILE.parent.mkdir(parents=True, exist_ok=True)
 
     default_config = apply_schema_defaults(WEIGHTS_SCHEMA, {})
@@ -418,15 +355,7 @@ def create_default_weights_file() -> Path:
 
 
 def get_weights_file_path(spec_dir: Path | None = None) -> Path:
-    """Get the appropriate weights file path based on validation.
-
-    Args:
-        spec_dir: Directory containing potential gandalf-weights.yaml
-                 (optional, defaults to SPEC_WEIGHTS_FILE)
-
-    Returns:
-        Path to the weights file to use (override, spec, or default)
-    """
+    """Get the appropriate weights file path based on validation."""
     if WEIGHTS_FILE_OVERRIDE:
         override_path = Path(WEIGHTS_FILE_OVERRIDE)
         if override_path.exists():
@@ -472,29 +401,14 @@ def get_weights_file_path(spec_dir: Path | None = None) -> Path:
 
 
 def format_validation_errors(errors: list[ValidationError]) -> list[str]:
-    """Format validation errors into readable strings.
-
-    Args:
-        errors: List of ValidationError objects
-
-    Returns:
-        List of formatted error messages
-    """
+    """Format validation errors into readable strings."""
     return [str(error) for error in errors]
 
 
 def apply_schema_defaults(
     schema: dict[str, Any], data: dict[str, Any]
 ) -> dict[str, Any]:
-    """Apply default values from schema to configuration data.
-
-    Args:
-        schema: Schema with default values
-        data: Configuration data to apply defaults to
-
-    Returns:
-        Configuration data with defaults applied
-    """
+    """Apply default values from schema to configuration data."""
 
     def apply_defaults_recursive(
         schema_def: dict[str, Any], config_data: dict[str, Any]
@@ -521,14 +435,7 @@ def apply_schema_defaults(
 def validate_weights_config(
     config: dict[str, Any],
 ) -> tuple[bool, list[str], dict[str, Any]]:
-    """Validate weights configuration using custom schema validation.
-
-    Args:
-        config: Configuration dictionary to validate
-
-    Returns:
-        Tuple of (is_valid, error_messages, normalized_config)
-    """
+    """Validate weights configuration using custom schema validation."""
     validator = GandalfSchemaValidator(WEIGHTS_SCHEMA, allow_unknown=False)
 
     # Validate the configuration
@@ -551,24 +458,12 @@ def validate_weights_config(
 
 
 def get_weights_schema() -> dict[str, Any]:
-    """Get the schema for weights configuration.
-
-    Returns:
-        The weights configuration schema
-    """
+    """Get the schema for weights configuration."""
     return WEIGHTS_SCHEMA
 
 
 def create_validator(
     schema: dict[str, Any], allow_unknown: bool = False
 ) -> GandalfSchemaValidator:
-    """Create a Gandalf schema validator instance.
-
-    Args:
-        schema: Schema dictionary
-        allow_unknown: Whether to allow unknown fields
-
-    Returns:
-        GandalfSchemaValidator instance
-    """
+    """Create a Gandalf schema validator instance."""
     return GandalfSchemaValidator(schema, allow_unknown=allow_unknown)
diff --git a/server/tests/core/test_server.py b/server/tests/core/test_server.py
index 393bd64..b50155f 100644
--- a/server/tests/core/test_server.py
+++ b/server/tests/core/test_server.py
@@ -6,6 +6,7 @@ import pytest
 
 from src.config.enums import ErrorCodes
 from src.core.server import GandalfMCP
+from src.utils.database_pool import DatabaseService
 
 
 class TestGandalfMCPInputValidation:
@@ -15,11 +16,19 @@ class TestGandalfMCPInputValidation:
         """Test that valid project root is accepted."""
         server = GandalfMCP(project_root=str(temp_project_dir))
         assert server.project_root is not None
+        # Test database service is initialized
+        assert server.db_service is not None
+        assert server.db_service.is_initialized()
+        server.shutdown()
 
     def test_none_project_root(self):
         """Test that None project root is accepted."""
         server = GandalfMCP(project_root=None)
         assert server.project_root is not None
+        # Test database service is initialized
+        assert server.db_service is not None
+        assert server.db_service.is_initialized()
+        server.shutdown()
 
     def test_empty_project_root_raises_error(self):
         """Test that empty project root raises ValueError."""
@@ -41,25 +50,35 @@ class TestGandalfMCPInputValidation:
         """Test that server initializes without explicit IDE parameter (removed)."""
         server = GandalfMCP()
         assert server.project_root is not None
+        assert server.db_service is not None
+        assert server.db_service.is_initialized()
+        server.shutdown()
 
     def test_none_explicit_ide(self):
         """Test that server initializes without explicit IDE parameter (removed)."""
         server = GandalfMCP()
         assert server.project_root is not None
+        assert server.db_service is not None
+        assert server.db_service.is_initialized()
+        server.shutdown()
 
 
 class TestGandalfMCPInitialization:
     """Test GandalfMCP server initialization."""
 
-    def setUp(self):
-        """Set up test fixtures."""
-
     def test_init_with_project_root(self, temp_project_dir):
         """Test initialization with explicit project root."""
         server = GandalfMCP(project_root=str(temp_project_dir))
 
         assert server.project_root.resolve() == temp_project_dir.resolve()
         assert server.is_ready is False
+        
+        # Test database service integration
+        assert server.db_service is not None
+        assert isinstance(server.db_service, DatabaseService)
+        assert server.db_service.is_initialized()
+        
+        server.shutdown()
 
     def test_init_without_project_root(self):
         """Test initialization without explicit project root."""
@@ -67,6 +86,13 @@ class TestGandalfMCPInitialization:
 
         assert server.project_root is not None
         assert server.is_ready is False
+        
+        # Test database service integration
+        assert server.db_service is not None
+        assert isinstance(server.db_service, DatabaseService)
+        assert server.db_service.is_initialized()
+        
+        server.shutdown()
 
     def test_init_handlers_setup(self):
         """Test that all required handlers are set up."""
@@ -80,6 +106,110 @@ class TestGandalfMCPInitialization:
         }
 
         assert set(server.handlers.keys()) == expected_handlers
+        server.shutdown()
+
+    def test_database_service_initialization(self):
+        """Test that database service is properly initialized during server creation."""
+        server = GandalfMCP()
+        
+        # Database service should be initialized
+        assert server.db_service is not None
+        assert server.db_service.is_initialized()
+        
+        # Should be able to get pool stats
+        stats = server.db_service.get_pool_stats()
+        assert isinstance(stats, dict)
+        
+        server.shutdown()
+
+    def test_server_shutdown(self, temp_db):
+        """Test that server shutdown properly closes database service."""
+        server = GandalfMCP()
+        
+        # Use database service to create a connection
+        with server.db_service.get_connection(temp_db) as conn:
+            conn.execute("SELECT 1")
+        
+        # Verify service is initialized and has connections
+        assert server.db_service.is_initialized()
+        stats = server.db_service.get_pool_stats()
+        assert str(temp_db) in stats
+        
+        # Shutdown server
+        server.shutdown()
+        
+        # Database service should be shut down
+        assert not server.db_service.is_initialized()
+        assert server.db_service.get_pool_stats() == {}
+
+    def test_server_shutdown_idempotent(self):
+        """Test that server shutdown can be called multiple times safely."""
+        server = GandalfMCP()
+        
+        assert server.db_service.is_initialized()
+        
+        # First shutdown
+        server.shutdown()
+        assert not server.db_service.is_initialized()
+        
+        # Second shutdown should not error
+        server.shutdown()
+        assert not server.db_service.is_initialized()
+
+
+class TestDatabaseServiceIntegration:
+    """Test database service integration with server tools."""
+
+    def test_tools_can_access_database_service(self, temp_db):
+        """Test that tools can access database service through server instance."""
+        server = GandalfMCP()
+        
+        try:
+            # Simulate tool call that uses database service
+            with server.db_service.get_connection(temp_db) as conn:
+                cursor = conn.cursor()
+                cursor.execute("CREATE TABLE test_table (id INTEGER, value TEXT)")
+                cursor.execute("INSERT INTO test_table VALUES (1, 'test')")
+                cursor.execute("SELECT value FROM test_table WHERE id = 1")
+                result = cursor.fetchone()
+                assert result[0] == "test"
+        finally:
+            server.shutdown()
+
+    def test_tool_handlers_receive_server_instance(self):
+        """Test that tool handlers receive server instance with database service."""
+        mock_tool = mock.Mock(
+            return_value={"content": [{"type": "text", "text": "result"}]}
+        )
+
+        server = GandalfMCP()
+        
+        try:
+            # Mock the tool handlers directly on the server instance
+            server.tool_handlers = {"test_tool": mock_tool}
+
+            request = {
+                "jsonrpc": "2.0",
+                "id": "test",
+                "method": "tools/call",
+                "params": {"name": "test_tool", "arguments": {"arg": "value"}},
+            }
+
+            response = server.handle_request(request)
+
+            assert "result" in response
+            assert "content" in response["result"]
+            
+            # Verify tool was called with server_instance containing database service
+            mock_tool.assert_called_once()
+            call_args, call_kwargs = mock_tool.call_args
+            
+            assert "server_instance" in call_kwargs
+            assert call_kwargs["server_instance"] is server
+            assert hasattr(call_kwargs["server_instance"], "db_service")
+            assert call_kwargs["server_instance"].db_service.is_initialized()
+        finally:
+            server.shutdown()
 
 
 class TestProjectRootDetection:
@@ -88,7 +218,10 @@ class TestProjectRootDetection:
     def test_server_project_root_resolution(self, temp_project_dir):
         """Test that server resolves project root correctly."""
         server = GandalfMCP(project_root=str(temp_project_dir))
-        assert server.project_root.resolve() == temp_project_dir.resolve()
+        try:
+            assert server.project_root.resolve() == temp_project_dir.resolve()
+        finally:
+            server.shutdown()
 
     def test_server_project_root_with_git(self, temp_project_dir):
         """Test server finds git repository root."""
@@ -104,12 +237,15 @@ class TestProjectRootDetection:
         import os
 
         original_cwd = os.getcwd()
+        server = None
         try:
             os.chdir(str(sub_dir))
             server = GandalfMCP()
             assert server.project_root.resolve() == temp_project_dir.resolve()
         finally:
             os.chdir(original_cwd)
+            if server:
+                server.shutdown()
 
 
 class TestMCPProtocolHandling:
@@ -118,37 +254,46 @@ class TestMCPProtocolHandling:
     def test_handle_initialize_request(self):
         """Test initialize request handling."""
         server = GandalfMCP()
-        request = {"method": "initialize", "id": "1"}
+        try:
+            request = {"method": "initialize", "id": "1"}
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response["jsonrpc"] == "2.0"
-        assert response["id"] == "1"
-        assert "protocolVersion" in response["result"]
-        assert "capabilities" in response["result"]
-        assert "serverInfo" in response["result"]
+            assert response["jsonrpc"] == "2.0"
+            assert response["id"] == "1"
+            assert "protocolVersion" in response["result"]
+            assert "capabilities" in response["result"]
+            assert "serverInfo" in response["result"]
+        finally:
+            server.shutdown()
 
     def test_handle_tools_list_request(self):
         """Test tools/list request handling."""
         server = GandalfMCP()
-        request = {"method": "tools/list", "id": "2"}
+        try:
+            request = {"method": "tools/list", "id": "2"}
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response["jsonrpc"] == "2.0"
-        assert response["id"] == "2"
-        assert "tools" in response["result"]
-        assert len(response["result"]["tools"]) > 0
+            assert response["jsonrpc"] == "2.0"
+            assert response["id"] == "2"
+            assert "tools" in response["result"]
+            assert len(response["result"]["tools"]) > 0
+        finally:
+            server.shutdown()
 
     def test_handle_notifications_initialized(self):
         """Test notifications/initialized handling."""
         server = GandalfMCP()
-        request = {"method": "notifications/initialized"}  # No id = notification
+        try:
+            request = {"method": "notifications/initialized"}  # No id = notification
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response is None  # Notifications don't return responses
-        assert server.is_ready is True
+            assert response is None  # Notifications don't return responses
+            assert server.is_ready is True
+        finally:
+            server.shutdown()
 
     def test_handle_tools_call_request(self):
         """Test handling tools/call requests."""
@@ -157,54 +302,66 @@ class TestMCPProtocolHandling:
         )
 
         server = GandalfMCP()
-        # Mock the tool handlers directly on the server instance
-        server.tool_handlers = {"test_tool": mock_tool}
+        try:
+            # Mock the tool handlers directly on the server instance
+            server.tool_handlers = {"test_tool": mock_tool}
 
-        request = {
-            "jsonrpc": "2.0",
-            "id": "test",
-            "method": "tools/call",
-            "params": {"name": "test_tool", "arguments": {"arg": "value"}},
-        }
+            request = {
+                "jsonrpc": "2.0",
+                "id": "test",
+                "method": "tools/call",
+                "params": {"name": "test_tool", "arguments": {"arg": "value"}},
+            }
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert "result" in response
-        assert "content" in response["result"]
-        mock_tool.assert_called_once()
+            assert "result" in response
+            assert "content" in response["result"]
+            mock_tool.assert_called_once()
+        finally:
+            server.shutdown()
 
     def test_handle_unknown_method(self):
         """Test handling of unknown methods."""
         server = GandalfMCP()
-        request = {"method": "unknown_method", "id": "4"}
+        try:
+            request = {"method": "unknown_method", "id": "4"}
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response["jsonrpc"] == "2.0"
-        assert response["id"] == "4"
-        assert response["error"]["code"] == ErrorCodes.METHOD_NOT_FOUND
-        assert "Method not found" in response["error"]["message"]
+            assert response["jsonrpc"] == "2.0"
+            assert response["id"] == "4"
+            assert response["error"]["code"] == ErrorCodes.METHOD_NOT_FOUND
+            assert "Method not found" in response["error"]["message"]
+        finally:
+            server.shutdown()
 
     def test_handle_malformed_request_no_method(self):
         """Test handling of malformed requests without method."""
         server = GandalfMCP()
-        request = {"id": "5"}  # Missing method
+        try:
+            request = {"id": "5"}  # Missing method
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response["jsonrpc"] == "2.0"
-        assert response["id"] == "5"
-        assert response["error"]["code"] == ErrorCodes.INVALID_REQUEST
-        assert "missing method" in response["error"]["message"]
+            assert response["jsonrpc"] == "2.0"
+            assert response["id"] == "5"
+            assert response["error"]["code"] == ErrorCodes.INVALID_REQUEST
+            assert "missing method" in response["error"]["message"]
+        finally:
+            server.shutdown()
 
     def test_handle_notification_unknown_method(self):
         """Test notification with unknown method returns None."""
         server = GandalfMCP()
-        request = {"method": "unknown_notification"}  # No id = notification
+        try:
+            request = {"method": "unknown_notification"}  # No id = notification
 
-        response = server.handle_request(request)
+            response = server.handle_request(request)
 
-        assert response is None
+            assert response is None
+        finally:
+            server.shutdown()
 
 
 class TestToolCallHandling:
@@ -283,25 +440,32 @@ class TestProjectRootUpdating:
     def test_project_root_set_during_initialization(self):
         """Test that project root is set during initialization."""
         server = GandalfMCP(project_root="/explicit/path")
-
-        # Project root should be resolved during initialization
-        assert server.project_root is not None
-        # The adapter may resolve the path differently, but it should be set
+        try:
+            # Project root should be resolved during initialization
+            assert server.project_root is not None
+            # The adapter may resolve the path differently, but it should be set
+        finally:
+            server.shutdown()
 
     def test_project_root_dynamic_detection(self):
         """Test dynamic project root detection."""
         server = GandalfMCP()
-
-        # Project root should be automatically detected
-        assert server.project_root is not None
+        try:
+            # Project root should be automatically detected
+            assert server.project_root is not None
+        finally:
+            server.shutdown()
 
     def test_project_root_consistency(self):
         """Test project root remains consistent."""
         server = GandalfMCP()
-        original_root = server.project_root
+        try:
+            original_root = server.project_root
 
-        # Project root should remain stable
-        assert server.project_root == original_root
+            # Project root should remain stable
+            assert server.project_root == original_root
+        finally:
+            server.shutdown()
 
 
 class TestComponentSetup:
@@ -310,27 +474,35 @@ class TestComponentSetup:
     def test_server_initialization_success(self):
         """Test that server initializes successfully with all components."""
         server = GandalfMCP()
-
-        # Server should initialize without exceptions
-        assert server.is_ready is False
-        assert server.project_root is not None
-        assert len(server.tool_handlers) > 0
-        assert len(server.tool_definitions) > 0
+        try:
+            # Server should initialize without exceptions
+            assert server.is_ready is False
+            assert server.project_root is not None
+            assert len(server.tool_handlers) > 0
+            assert len(server.tool_definitions) > 0
+            
+            # Database service should be initialized
+            assert server.db_service is not None
+            assert server.db_service.is_initialized()
+        finally:
+            server.shutdown()
 
     def test_server_ready_state_handling(self):
         """Test server ready state handling."""
         server = GandalfMCP()
+        try:
+            # Initially not ready
+            assert server.is_ready is False
 
-        # Initially not ready
-        assert server.is_ready is False
-
-        # Send initialized notification
-        request = {"method": "notifications/initialized"}
-        response = server.handle_request(request)
+            # Send initialized notification
+            request = {"method": "notifications/initialized"}
+            response = server.handle_request(request)
 
-        # Should be ready now
-        assert response is None  # Notifications return None
-        assert server.is_ready is True
+            # Should be ready now
+            assert response is None  # Notifications return None
+            assert server.is_ready is True
+        finally:
+            server.shutdown()
 
 
 class TestRequestHandlingEdgeCases:
@@ -339,39 +511,43 @@ class TestRequestHandlingEdgeCases:
     def test_handle_request_exception_in_handler(self):
         """Test exception handling within request handlers."""
         server = GandalfMCP()
+        try:
+            # Replace the handler in the handlers dictionary to raise an exception
+            original_handler = server.handlers["initialize"]
 
-        # Replace the handler in the handlers dictionary to raise an exception
-        original_handler = server.handlers["initialize"]
-
-        def failing_handler(request):
-            raise ValueError("Handler error")
+            def failing_handler(request):
+                raise ValueError("Handler error")
 
-        server.handlers["initialize"] = failing_handler
+            server.handlers["initialize"] = failing_handler
 
-        try:
-            request = {"method": "initialize", "id": "1"}
-            response = server.handle_request(request)
+            try:
+                request = {"method": "initialize", "id": "1"}
+                response = server.handle_request(request)
 
-            assert "error" in response
-            assert response["error"]["code"] == ErrorCodes.INTERNAL_ERROR
-            assert "Internal error" in response["error"]["message"]
+                assert "error" in response
+                assert response["error"]["code"] == ErrorCodes.INTERNAL_ERROR
+                assert "Internal error" in response["error"]["message"]
+            finally:
+                # Restore original handler
+                server.handlers["initialize"] = original_handler
         finally:
-            # Restore original handler
-            server.handlers["initialize"] = original_handler
+            server.shutdown()
 
     def test_handle_request_exception_in_notification_handler(self):
         """Test exception in notification handler returns None."""
         server = GandalfMCP()
-
-        with mock.patch.object(
-            server,
-            "_notifications_initialized",
-            side_effect=ValueError("Handler error"),
-        ):
-            request = {"method": "notifications/initialized"}  # Notification
-            response = server.handle_request(request)
-
-            assert response is None
+        try:
+            with mock.patch.object(
+                server,
+                "_notifications_initialized",
+                side_effect=ValueError("Handler error"),
+            ):
+                request = {"method": "notifications/initialized"}  # Notification
+                response = server.handle_request(request)
+
+                assert response is None
+        finally:
+            server.shutdown()
 
     def test_handle_request_top_level_exception(self):
         """Test top-level exception handling."""
@@ -407,26 +583,28 @@ class TestIntegrationScenarios:
     def test_full_initialization_flow(self, temp_project_dir):
         """Test complete initialization flow."""
         server = GandalfMCP(project_root=str(temp_project_dir))
+        try:
+            # Initialize
+            init_request = {"method": "initialize", "id": "1"}
+            init_response = server.handle_request(init_request)
 
-        # Initialize
-        init_request = {"method": "initialize", "id": "1"}
-        init_response = server.handle_request(init_request)
-
-        assert init_response["result"]["protocolVersion"]
-        assert not server.is_ready
+            assert init_response["result"]["protocolVersion"]
+            assert not server.is_ready
 
-        # Send initialized notification
-        notif_request = {"method": "notifications/initialized"}
-        notif_response = server.handle_request(notif_request)
+            # Send initialized notification
+            notif_request = {"method": "notifications/initialized"}
+            notif_response = server.handle_request(notif_request)
 
-        assert notif_response is None
-        assert server.is_ready
+            assert notif_response is None
+            assert server.is_ready
 
-        # List tools
-        tools_request = {"method": "tools/list", "id": "2"}
-        tools_response = server.handle_request(tools_request)
+            # List tools
+            tools_request = {"method": "tools/list", "id": "2"}
+            tools_response = server.handle_request(tools_request)
 
-        assert len(tools_response["result"]["tools"]) > 0
+            assert len(tools_response["result"]["tools"]) > 0
+        finally:
+            server.shutdown()
 
 
 class TestPerformanceTracking:
@@ -494,3 +672,28 @@ def sample_requests():
             "params": {"name": "test_tool", "arguments": {}},
         },
     }
+
+
+@pytest.fixture
+def temp_db():
+    """Create a temporary SQLite database for testing."""
+    import tempfile
+    import sqlite3
+    from pathlib import Path
+    
+    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as f:
+        db_path = Path(f.name)
+
+    # Initialize database with basic table
+    with sqlite3.connect(str(db_path)) as conn:
+        conn.execute("CREATE TABLE test (id INTEGER PRIMARY KEY, value TEXT)")
+        conn.execute("INSERT INTO test (value) VALUES ('test_data')")
+        conn.commit()
+
+    yield db_path
+
+    # Cleanup
+    try:
+        db_path.unlink()
+    except FileNotFoundError:
+        pass
diff --git a/server/tests/test_main.py b/server/tests/test_main.py
index 0a887ad..aeec6e9 100644
--- a/server/tests/test_main.py
+++ b/server/tests/test_main.py
@@ -28,6 +28,7 @@ class TestMain:
 
                 mock_gandalf.assert_called_once_with(project_root=None)
                 mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_with_valid_project_root(self, tmp_path):
         """Test main function with valid project root path."""
@@ -45,6 +46,7 @@ class TestMain:
 
                 mock_gandalf.assert_called_once_with(project_root=hobbiton_path)
                 mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_with_nonexistent_project_root(self, tmp_path, capsys):
         """Test main function with non-existent project root."""
@@ -62,6 +64,7 @@ class TestMain:
                 # Server should start successfully with nonexistent project root
                 mock_gandalf.assert_called_once_with(project_root=isengard_path)
                 mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_with_file_as_project_root(self, tmp_path, capsys):
         """Test main function with file instead of directory."""
@@ -143,6 +146,45 @@ class TestMain:
                 captured = capsys.readouterr()
                 assert "Error: Failed to start server" in captured.err
                 assert "Server runtime error" in captured.err
+                
+                # Verify shutdown was called even though run failed
+                mock_server.shutdown.assert_called_once()
+
+    def test_main_server_shutdown_on_keyboard_interrupt(self, tmp_path):
+        """Test that server shutdown is called on KeyboardInterrupt during run."""
+        gondor_path = tmp_path / "gondor"
+        gondor_path.mkdir()
+
+        with patch("sys.argv", ["gandalf-server", "--project-root", str(gondor_path)]):
+            with patch("src.main.GandalfMCP") as mock_gandalf:
+                mock_server = Mock()
+                mock_server.run.side_effect = KeyboardInterrupt("User interrupted")
+                mock_gandalf.return_value = mock_server
+
+                with pytest.raises(SystemExit) as exc_info:
+                    main()
+
+                assert exc_info.value.code == 1
+                # Verify shutdown was called even with KeyboardInterrupt
+                mock_server.shutdown.assert_called_once()
+
+    def test_main_server_shutdown_on_exception_during_run(self, tmp_path):
+        """Test that server shutdown is called when run() raises exception."""
+        rohan_path = tmp_path / "rohan" 
+        rohan_path.mkdir()
+
+        with patch("sys.argv", ["gandalf-server", "--project-root", str(rohan_path)]):
+            with patch("src.main.GandalfMCP") as mock_gandalf:
+                mock_server = Mock()
+                mock_server.run.side_effect = RuntimeError("Unexpected runtime error")
+                mock_gandalf.return_value = mock_server
+
+                with pytest.raises(SystemExit):
+                    main()
+
+                # Verify both run and shutdown were called
+                mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_path_resolution(self, tmp_path):
         """Test that project root path is properly resolved."""
@@ -160,6 +202,10 @@ class TestMain:
                 assert called_path.is_absolute()
                 assert called_path.name == "moria"
                 assert called_path == moria_path
+                
+                # Verify normal flow includes shutdown
+                mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_empty_project_root_argument(self, capsys):
         """Test main function with empty project root argument."""
@@ -172,6 +218,7 @@ class TestMain:
 
                 mock_gandalf.assert_called_once_with(project_root=None)
                 mock_server.run.assert_called_once()
+                mock_server.shutdown.assert_called_once()
 
     def test_main_with_os_error_on_path_resolution(self, capsys):
         """Test main function when Path operations raise OSError."""
diff --git a/server/tests/utils/test_database_pool.py b/server/tests/utils/test_database_pool.py
index 841d490..2035cf0 100644
--- a/server/tests/utils/test_database_pool.py
+++ b/server/tests/utils/test_database_pool.py
@@ -12,6 +12,7 @@ from unittest.mock import patch
 import pytest
 from src.utils.database_pool import (
     ConnectionPool,
+    DatabaseService,
     close_database_pool,
     get_database_connection,
     get_database_pool,
@@ -27,7 +28,6 @@ class TestConnectionPool:
         assert pool.max_connections == 3
         assert pool.timeout == 1.0
         assert len(pool._pools) == 0
-        assert len(pool._pools) == 0
 
     def test_get_connection_creates_new(self, temp_db):
         """Test that get_connection creates new connection when pool is empty."""
@@ -58,7 +58,6 @@ class TestConnectionPool:
         pool = ConnectionPool(max_connections=2)
 
         # Create connections to fill up the pool beyond limit
-        connections = []
         for _ in range(3):
             with pool.get_connection(temp_db) as conn:
                 # Do something to ensure connection is used
@@ -131,7 +130,6 @@ class TestConnectionPool:
 
         # Pool should be empty
         assert len(pool._pools) == 0
-        assert len(pool._pools) == 0
 
     def test_get_pool_stats(self, temp_db):
         """Test getting pool statistics."""
@@ -175,27 +173,196 @@ class TestConnectionPool:
             temp_db2.unlink()
 
 
-class TestGlobalPool:
-    """Test the global connection pool functions."""
+class TestDatabaseService:
+    """Test the DatabaseService class."""
+
+    def test_database_service_init(self):
+        """Test DatabaseService initialization."""
+        service = DatabaseService()
+        assert not service.is_initialized()
+        assert service.get_pool_stats() == {}
+
+    def test_database_service_initialize(self):
+        """Test DatabaseService initialization."""
+        service = DatabaseService()
+        service.initialize(max_connections=3, timeout=1.5)
+        
+        assert service.is_initialized()
+        assert service._pool is not None
+        assert service._pool.max_connections == 3
+        assert service._pool.timeout == 1.5
+
+    def test_database_service_initialize_idempotent(self):
+        """Test that initialize can be called multiple times safely."""
+        service = DatabaseService()
+        service.initialize(max_connections=2)
+        
+        pool1 = service._pool
+        
+        # Second initialize should not create new pool
+        service.initialize(max_connections=5)
+        pool2 = service._pool
+        
+        assert pool1 is pool2
+        assert pool1.max_connections == 2  # Original settings preserved
+
+    def test_database_service_get_connection_not_initialized(self):
+        """Test that get_connection raises error when not initialized."""
+        service = DatabaseService()
+        
+        with pytest.raises(RuntimeError, match="DatabaseService not initialized"):
+            with service.get_connection(Path("test.db")):
+                pass
+
+    def test_database_service_get_connection_success(self, temp_db):
+        """Test successful database connection through service."""
+        service = DatabaseService()
+        service.initialize()
+        
+        with service.get_connection(temp_db) as conn:
+            assert isinstance(conn, sqlite3.Connection)
+            cursor = conn.cursor()
+            cursor.execute("SELECT 1")
+            assert cursor.fetchone()[0] == 1
+
+    def test_database_service_connection_reuse(self, temp_db):
+        """Test that service reuses connections."""
+        service = DatabaseService()
+        service.initialize()
+        
+        # First connection
+        with service.get_connection(temp_db) as conn1:
+            conn1_id = id(conn1)
+
+        # Second connection should be reused
+        with service.get_connection(temp_db) as conn2:
+            conn2_id = id(conn2)
+            assert conn1_id == conn2_id
+
+    def test_database_service_get_pool_stats(self, temp_db):
+        """Test getting pool statistics from service."""
+        service = DatabaseService()
+        service.initialize()
+        
+        # Initially empty
+        stats = service.get_pool_stats()
+        assert len(stats) == 0
+        
+        # Use connection
+        with service.get_connection(temp_db) as conn:
+            conn.execute("SELECT 1")
+            
+        # Should have stats
+        stats = service.get_pool_stats()
+        assert str(temp_db) in stats
+
+    def test_database_service_shutdown(self, temp_db):
+        """Test service shutdown closes all connections."""
+        service = DatabaseService()
+        service.initialize()
+        
+        # Use connection to add to pool
+        with service.get_connection(temp_db) as conn:
+            conn.execute("SELECT 1")
+            
+        # Verify connection is in pool
+        stats = service.get_pool_stats()
+        assert str(temp_db) in stats
+        
+        # Shutdown
+        service.shutdown()
+        
+        assert not service.is_initialized()
+        assert service._pool is None
+        assert service.get_pool_stats() == {}
+
+    def test_database_service_shutdown_idempotent(self):
+        """Test that shutdown can be called multiple times safely."""
+        service = DatabaseService()
+        service.initialize()
+        
+        # First shutdown
+        service.shutdown()
+        assert not service.is_initialized()
+        
+        # Second shutdown should not error
+        service.shutdown()
+        assert not service.is_initialized()
+
+    def test_database_service_concurrent_access(self, temp_db):
+        """Test thread-safe concurrent access to database service."""
+        service = DatabaseService()
+        service.initialize()
+        
+        results = []
+        errors = []
+
+        def worker():
+            try:
+                with service.get_connection(temp_db) as conn:
+                    cursor = conn.cursor()
+                    cursor.execute("SELECT 1")
+                    result = cursor.fetchone()[0]
+                    results.append(result)
+                    time.sleep(0.01)
+            except Exception as e:
+                errors.append(e)
+
+        # Start multiple threads
+        threads = []
+        for _ in range(5):
+            thread = threading.Thread(target=worker)
+            threads.append(thread)
+            thread.start()
+
+        # Wait for all threads
+        for thread in threads:
+            thread.join()
+
+        # All operations should succeed
+        assert len(errors) == 0
+        assert len(results) == 5
+        assert all(r == 1 for r in results)
+
+    def test_database_service_error_handling(self):
+        """Test service error handling with invalid database path."""
+        service = DatabaseService()
+        service.initialize()
+        
+        invalid_path = Path("/invalid/path/database.db")
+        
+        with pytest.raises((sqlite3.Error, OSError)):
+            with service.get_connection(invalid_path):
+                pass
+
+
+class TestBackwardCompatibility:
+    """Test backward compatibility functions."""
+
+    def test_get_database_pool_returns_service(self):
+        """Test that get_database_pool returns DatabaseService instance."""
+        service = get_database_pool()
+        assert isinstance(service, DatabaseService)
+        assert service.is_initialized()
 
     def test_get_database_pool_singleton(self):
         """Test that get_database_pool returns singleton instance."""
-        pool1 = get_database_pool()
-        pool2 = get_database_pool()
-        assert pool1 is pool2
+        service1 = get_database_pool()
+        service2 = get_database_pool()
+        assert service1 is service2
 
     def test_close_database_pool(self):
         """Test closing the global database pool."""
-        # Get initial pool
-        pool = get_database_pool()
-        assert pool is not None
+        # Get initial service
+        service = get_database_pool()
+        assert service is not None
 
         # Close pool
         close_database_pool()
 
-        # New pool should be created
-        new_pool = get_database_pool()
-        assert new_pool is not pool
+        # New service should be created
+        new_service = get_database_pool()
+        assert new_service is not service
 
     def test_get_database_connection_context_manager(self, temp_db):
         """Test the global get_database_connection function."""
@@ -217,11 +384,32 @@ class TestGlobalPool:
 class TestPragmaSettings:
     """Test that SQLite PRAGMA settings are applied correctly."""
 
-    def test_pragma_settings_applied(self, temp_db):
-        """Test that PRAGMA settings are applied to new connections."""
-        pool = ConnectionPool()
+    def test_pragma_settings_applied_service(self, temp_db):
+        """Test that PRAGMA settings are applied through service."""
+        service = DatabaseService()
+        service.initialize()
 
-        with pool.get_connection(temp_db) as conn:
+        with service.get_connection(temp_db) as conn:
+            cursor = conn.cursor()
+
+            # Check foreign keys pragma
+            cursor.execute("PRAGMA foreign_keys")
+            foreign_keys = cursor.fetchone()[0]
+            assert foreign_keys == 1
+
+            # Check journal mode pragma
+            cursor.execute("PRAGMA journal_mode")
+            journal_mode = cursor.fetchone()[0]
+            assert journal_mode == "wal"
+
+            # Check synchronous pragma
+            cursor.execute("PRAGMA synchronous")
+            synchronous = cursor.fetchone()[0]
+            assert synchronous == 1  # NORMAL mode
+
+    def test_pragma_settings_applied_backward_compat(self, temp_db):
+        """Test that PRAGMA settings are applied via backward compatibility functions."""
+        with get_database_connection(temp_db) as conn:
             cursor = conn.cursor()
 
             # Check foreign keys pragma
